{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the model to local so it can be used again and again\n",
    "# !mkdir ./sentence_wise_email\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "\n",
    "# DO NOT DOWNLOAD IT EACH TIME, IF YOU HAD DOWNLOADED IT ONCE, ITS ENOUGH\n",
    "\n",
    "# !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC ./sentence_wise_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(\"./sentence_wise_email\")\n",
    "\n",
    "# Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join((str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The Hunger Games (The Hunger Games, #1)\n4\n"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./goodbooks-10k/books.csv\", usecols=['id', 'title'])\n",
    "titleList = df['title'].tolist()\n",
    "idList = df['id'].tolist()\n",
    "print (titleList[0])\n",
    "titleIdDict = dict(zip(titleList, idList))\n",
    "print(titleIdDict.get(titleList[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function so that one session can be called multiple times. \n",
    "#Useful while multiple calls need to be done for embedding.\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "\n",
    "embed_fn = embed_useT('./sentence_wise_email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     \"we are sorry for the inconvenience\",\n",
    "#     \"we are sorry for the delay\",\n",
    "#     \"we regret for your inconvenience\",\n",
    "#     \"we don't deliver to baner region in pune\",\n",
    "#     \"we will get you the best possible rate\"\n",
    "# ]\n",
    "\n",
    "# encoding_matrix = embed_fn(messages)\n",
    "encoding_matrix = embed_fn(titleList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.0000002 , 0.6433921 , 0.6784331 , ..., 0.21079348, 0.3800934 ,\n        0.42424315],\n       [0.6433921 , 0.9999998 , 0.72538793, ..., 0.14122126, 0.30932796,\n        0.2677622 ],\n       [0.6784331 , 0.72538793, 1.0000001 , ..., 0.14197785, 0.359599  ,\n        0.29009897],\n       ...,\n       [0.21079348, 0.14122126, 0.14197785, ..., 1.0000001 , 0.22863138,\n        0.47335958],\n       [0.3800934 , 0.30932796, 0.359599  , ..., 0.22863138, 0.9999999 ,\n        0.3064736 ],\n       [0.42424315, 0.2677622 , 0.29009897, ..., 0.47335958, 0.3064736 ,\n        0.9999999 ]], dtype=float32)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.inner(encoding_matrix, encoding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    "from decimal import Decimal\n",
    "\n",
    "class Similarity():\n",
    "    def euclidean_distance(self,x,y):\n",
    "\n",
    "        \"\"\" return euclidean distance between two lists \"\"\"\n",
    "\n",
    "        return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "    def manhattan_distance(self,x,y):\n",
    "\n",
    "        \"\"\" return manhattan distance between two lists \"\"\"\n",
    "\n",
    "        return sum(abs(a-b) for a,b in zip(x,y))\n",
    "\n",
    "    def minkowski_distance(self,x,y,p_value):\n",
    "\n",
    "        \"\"\" return minkowski distance between two lists \"\"\"\n",
    "\n",
    "        return self.nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),\n",
    "           p_value)\n",
    "\n",
    "    def nth_root(self,value, n_root):\n",
    "\n",
    "        \"\"\" returns the n_root of an value \"\"\"\n",
    "\n",
    "        root_value = 1/float(n_root)\n",
    "        return round (Decimal(value) ** Decimal(root_value),3)\n",
    "\n",
    "    def cosine_similarity(self,x,y):\n",
    "\n",
    "        \"\"\" return cosine similarity between two lists \"\"\"\n",
    "\n",
    "        numerator = sum(a*b for a,b in zip(x,y))\n",
    "        denominator = self.square_rooted(x)*self.square_rooted(y)\n",
    "        return round(numerator/float(denominator),3)\n",
    "\n",
    "    def square_rooted(self,x):\n",
    "\n",
    "        \"\"\" return 3 rounded square rooted value \"\"\"\n",
    "\n",
    "        return round(sqrt(sum([a*a for a in x])),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendListOfTopBooks(bookTitle):\n",
    "    similarities = []\n",
    "    measures = Similarity()\n",
    "    bookIndex = titleIdDict.get(titleList[3])\n",
    "    for index, title in enumerate(titleList):\n",
    "        if(index != bookIndex):\n",
    "            similarities.append(measures.cosine_similarity(encoding_matrix[bookIndex], encoding_matrix[index]))\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendTopBooks(bookTitle):\n",
    "    similarities = []\n",
    "    measures = Similarity()\n",
    "    bookIndex = titleIdDict.get(titleList[3])\n",
    "    for index, title in enumerate(titleList):\n",
    "        if(index != bookIndex):\n",
    "            similarities.append({'score': measures.cosine_similarity(encoding_matrix[bookIndex], encoding_matrix[index]), 'title': titleList[index]})\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.643"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = Similarity()\n",
    "measures.cosine_similarity(encoding_matrix[0],encoding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Book title to recommend:  The Hunger Games (The Hunger Games, #1)\n[{'score': 0.836, 'title': 'The Catcher in the Rye'}, {'score': 0.805, 'title': 'The Pursuit of Happyness'}, {'score': 0.797, 'title': 'The Silver Linings Playbook'}, {'score': 0.792, 'title': 'The Phantom Tollbooth'}, {'score': 0.787, 'title': 'Gone with the Wind'}, {'score': 0.784, 'title': 'To Kill a Mockingbird'}, {'score': 0.783, 'title': 'The Dispossessed'}, {'score': 0.78, 'title': 'Wuthering Heights'}, {'score': 0.776, 'title': 'The Perks of Being a Wallflower'}, {'score': 0.773, 'title': 'Pride and Prejudice'}]\n"
    }
   ],
   "source": [
    "bookTitle = titleList[0]\n",
    "print('Book title to recommend: ', bookTitle)\n",
    "similarities = recommendTopBooks(bookTitle)\n",
    "booksRecommended = sorted(similarities, key = lambda i: i['score'], reverse=True)\n",
    "print(booksRecommended[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tensorflow_env': conda)",
   "language": "python",
   "name": "python37664bittensorflowenvconda7abfa4292e63449e86f2f09e04d909cd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}